{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from mlxtend.data import loadlocal_mnist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images,train_labels=loadlocal_mnist(images_path='D:/Downloads/MNIST/raw/train-images-idx3-ubyte', labels_path='D:/Downloads/MNIST/raw/train-labels-idx1-ubyte')\n",
    "test_images,test_labels=loadlocal_mnist(images_path='D:/Downloads/MNIST/raw/t10k-images-idx3-ubyte', labels_path='D:/Downloads/MNIST/raw/t10k-labels-idx1-ubyte')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images,train_labels=loadlocal_mnist(images_path='D:/Downloads/MNIST/raw/train-images-idx3-ubyte', labels_path='D:/Downloads/MNIST/raw/train-labels-idx1-ubyte')\n",
    "# test_images,test_labels=loadlocal_mnist(images_path='D:/Downloads/MNIST/raw/t10k-images-idx3-ubyte', labels_path='D:/Downloads/MNIST/raw/t10k-labels-idx1-ubyte')\n",
    "\n",
    "# np.save('train_images',train_images)\n",
    "# np.save('train_labels',train_labels)\n",
    "# np.save('test_images',test_images)\n",
    "# np.save('test_labels',test_labels)\n",
    "\n",
    "# train_df=train_df.to_numpy()\n",
    "# test_df=test_df.to_numpy()\n",
    "# train_images = train_df[:, 1:]/255\n",
    "# test_images = test_df[:, 1:]/255\n",
    "# train_labels = train_df[:, 0]\n",
    "# test_labels = test_df[:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pre processing\n",
    "standardscalar = StandardScaler()\n",
    "\n",
    "train_images = standardscalar.fit_transform(train_images)\n",
    "test_images = standardscalar.transform(test_images)\n",
    "\n",
    "data=[]\n",
    "labels=[]\n",
    "for i in range(len(train_images)):\n",
    "    data.append(train_images[i])\n",
    "    labels.append(train_labels[i])\n",
    "for i in range(len(test_images)):\n",
    "    data.append(test_images[i])\n",
    "    labels.append(test_labels[i])\n",
    "\n",
    "data=np.array(data)\n",
    "labels=np.array(labels)\n",
    "\n",
    "#random shuffling\n",
    "data,labels=shuffle(data,labels)\n",
    "\n",
    "#dividing by max value to convert every pixel to 0-1\n",
    "data=data/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(data,labels, train,val,test):\n",
    "    \"\"\" a function that will get dataset and training dataset fraction as input and return x_train, x_test, y_train, y_test \"\"\"\n",
    "    \n",
    "    print(\"Total length is \"+str(len(data)))\n",
    "    \n",
    "    train_samples=len(data)*train//(train+test+val)\n",
    "    val_samples=len(data)*val//(train+test+val)\n",
    "    \n",
    "    train_data=data[:train_samples]\n",
    "    train_labels=labels[:train_samples]\n",
    "    \n",
    "    val_data=data[train_samples+1:train_samples+val_samples+1]\n",
    "    val_labels=labels[train_samples+1:train_samples+val_samples+1]\n",
    "    \n",
    "    test_data=data[train_samples+val_samples:]\n",
    "    test_labels=labels[train_samples+val_samples:]\n",
    "    \n",
    "    return train_data,train_labels,val_data,val_labels,test_data,test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length is 70000\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_val,y_val,x_test,y_test=train_val_test_split(data,labels,7,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork():\n",
    "    activations = ['relu','leaky_relu', 'sigmoid', 'linear', 'tanh', 'softmax']\n",
    "    weight_initializations = ['zero', 'random', 'normal']\n",
    "    \n",
    "    def __init__(self, n_layers, layer_sizes, activation, lr, weight_initialization, batch_size, epochs):\n",
    "        self.n_layers = n_layers\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation = activation\n",
    "        self.lr = lr\n",
    "        self.weight_initialization = weight_initialization\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.W,self.B=self.initialize_weights()  #Weight and Bias dictionary respectively\n",
    "        self.A={}   #Activation values\n",
    "        self.Z={}   #Pre activation values\n",
    "        self.W_grad,self.B_grad=self.initialize_weights()\n",
    "        self.A_grad={}\n",
    "        self.Z_grad={}\n",
    "        self.train_loss=[]\n",
    "        self.train_epochs=[]\n",
    "        self.val_epochs=[]\n",
    "        self.val_loss=[]\n",
    "        \n",
    "    def activate(self, arr, name, grad=False):\n",
    "        if(name=='relu'):\n",
    "            if(not grad):\n",
    "                arr[arr<0]=0\n",
    "                \n",
    "            else:\n",
    "                arr[arr<0]=0\n",
    "                arr[arr>0]=1\n",
    "                \n",
    "        elif(name=='leaky_relu'):\n",
    "            if(not grad):\n",
    "                for i in arr:\n",
    "                    if(i<0):\n",
    "                        i=i*0.01\n",
    "            else:\n",
    "                arr[arr<0]=0.01\n",
    "                \n",
    "        elif(name=='sigmoid'):\n",
    "            if(not grad):\n",
    "                arr=1/(1+np.exp(-arr))\n",
    "                \n",
    "            else:\n",
    "                x=self.activate(arr,'sigmoid')\n",
    "                arr=x*(1-x)\n",
    "        \n",
    "        elif(name=='linear'):\n",
    "            if(grad):\n",
    "                arr=np.ones(arr.shape)\n",
    "                \n",
    "        elif(name=='tanh'):\n",
    "            if(not grad):\n",
    "                arr=np.tanh(arr)\n",
    "            \n",
    "            else:\n",
    "                y=np.tanh(arr)\n",
    "                arr=1-y*y\n",
    "                \n",
    "        else:\n",
    "            #softmax derivative is a jacobian matrix\n",
    "            y = np.exp(arr)\n",
    "            arr= y/(np.sum(y,axis = 1, keepdims = True))\n",
    "            \n",
    "        \n",
    "        mini=1e-20\n",
    "        maxi=1e20\n",
    "        arr[arr<mini]=0\n",
    "        arr[arr>maxi]=maxi\n",
    "        return arr\n",
    "\n",
    "    def initialization(self, name, shape):\n",
    "        x=np.zeros(shape)\n",
    "        if(name=='random'):\n",
    "            x=np.random.rand(shape[0],shape[1])*0.01\n",
    "            \n",
    "        elif(name=='normal'):\n",
    "            x=np.random.normal(size = shape)*0.01\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        Weight={}\n",
    "        Bias={}\n",
    "        \n",
    "        for i in range(self.n_layers-1):\n",
    "            Weight[i+1]=self.initialization(self.weight_initialization, (self.layer_sizes[i],self.layer_sizes[i+1]))\n",
    "            Bias[i+1]=self.initialization(self.weight_initialization, (1,self.layer_sizes[i+1]))    \n",
    "        \n",
    "        return Weight,Bias\n",
    "    \n",
    "    def fit(self,x_train,y_train,x_val=None,y_val=None):\n",
    "        val_flag=False\n",
    "        if(x_val is not None):\n",
    "            val_flag=True\n",
    "            val_labels_vector=np.zeros((len(x_val),10))\n",
    "            for i in range(len(x_val)):\n",
    "                val_labels_vector[i][y_val[i]]=1\n",
    "                \n",
    "        for e in range(self.epochs):\n",
    "            \n",
    "            print(\"Epoch \"+str(e))\n",
    "            curr_loss=0\n",
    "            curr_val_loss=0\n",
    "            sample_ind=0\n",
    "            self.train_epochs.append(e+1)\n",
    "            times=0\n",
    "            \n",
    "            if(x_val is not None):\n",
    "                self.val_epochs.append(e+1)\n",
    "                \n",
    "            while sample_ind<len(x_train):\n",
    "                times+=1\n",
    "                \n",
    "                curr_batch=x_train[sample_ind:min(len(x_train),sample_ind+self.batch_size)]\n",
    "                curr_labels=y_train[sample_ind:min(len(y_train),sample_ind+self.batch_size)]\n",
    "                \n",
    "                #forward prop\n",
    "                self.A[0]=curr_batch\n",
    "                self.forward_propagation()\n",
    "                \n",
    "                #loss calc\n",
    "                labels_vectorized=np.zeros((len(curr_labels),10))\n",
    "                for i in range(len(curr_labels)):\n",
    "                    labels_vectorized[i][curr_labels[i]]=1\n",
    "                \n",
    "                #loss\n",
    "                loss=self.cross_entropy_loss(self.A[self.n_layers-1],labels_vectorized)\n",
    "                curr_loss+=loss\n",
    "                \n",
    "                output=self.A[self.n_layers-1]\n",
    "                new_arr=output*(1-output)\n",
    "                self.A_grad[self.n_layers-1]=(output-labels_vectorized)/new_arr\n",
    "                \n",
    "                #backward pass for all layers\n",
    "                self.backward_propagation()\n",
    "                    \n",
    "                #weight update for all layers\n",
    "                for l in range(1,self.n_layers):\n",
    "                    self.W[l]=self.W[l]-self.lr*self.W_grad[l]\n",
    "                    self.B[l]=self.B[l]-self.lr*self.B_grad[l]\n",
    "                \n",
    "                sample_ind+=self.batch_size\n",
    "                \n",
    "            if(x_val is not None):\n",
    "                findd=self.predict_proba(x_val)\n",
    "                curr_val_loss=self.cross_entropy_loss(findd,val_labels_vector)\n",
    "\n",
    "            print(\"Training Loss of epoch \"+str(e)+\" is \"+str(curr_loss/times))\n",
    "            if(val_flag):\n",
    "                print(\"Validation Loss of epoch \"+str(e)+\" is \"+str(curr_val_loss))\n",
    "\n",
    "            self.train_loss.append(curr_loss/times)\n",
    "            if(x_val is not None):\n",
    "                self.val_loss.append(curr_val_loss)\n",
    "        \n",
    "        self.plot_training_val(val_flag)\n",
    "        \n",
    "    def plot_training_val(self,val_flag):\n",
    "        plt.plot(self.train_epochs,self.train_loss,'b',label='training')\n",
    "        plt.plot(self.val_epochs,self.val_loss,'r',label='validation')\n",
    "        plt.title(\"Loss vs epochs for \"+self.activation+\" for learning rate \"+str(self.lr))\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "            \n",
    "    def forward_propagation(self):\n",
    "\n",
    "        for l in range(1,self.n_layers):\n",
    "            self.Z[l]=np.dot(self.A[l-1],self.W[l])\n",
    "            if(l!=1):\n",
    "                self.Z[l]+=self.B[l]\n",
    "\n",
    "            self.A[l]=self.activate(self.Z[l],self.activation)\n",
    "        \n",
    "        #applying softmax on last layer as loss is cross entropy and we might end up taking log 0\n",
    "        self.A[l]=self.activate(self.A[l],'softmax')\n",
    "        \n",
    "    def backward_propagation(self):\n",
    "\n",
    "        for l in range(self.n_layers-1,0,-1):\n",
    "            self.Z_grad[l]=self.activate(self.Z[l],self.activation,grad=True)*self.A_grad[l]\n",
    "            assert(self.Z_grad[l].shape==self.Z[l].shape)\n",
    "            self.W_grad[l]=np.dot(self.A[l-1].T,self.Z_grad[l])/len(self.A_grad[l])\n",
    "            assert(self.W_grad[l].shape==self.W[l].shape)\n",
    "            self.B_grad[l]=np.sum(self.Z_grad[l],axis=0)/len(self.A_grad[l])\n",
    "            self.B_grad[l]=self.B_grad[l].reshape((self.B[l].shape))\n",
    "            assert(self.B_grad[l].shape==self.B[l].shape)\n",
    "            self.A_grad[l-1]=np.dot(self.Z_grad[l],self.W[l].T)\n",
    "            assert(self.A_grad[l-1].shape==self.A[l-1].shape)\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        print(\"pred\")\n",
    "        preact={}\n",
    "        act={}\n",
    "        act[0]=X\n",
    "        \n",
    "        for l in range(1,self.n_layers):\n",
    "            preact[l]=np.dot(act[l-1],self.W[l])\n",
    "            if(l!=1):\n",
    "                preact[l]+=self.B[l]\n",
    "                \n",
    "            act[l]=self.activate(preact[l],self.activation)\n",
    "            \n",
    "        ans=self.activate(act[self.n_layers-1],'softmax')\n",
    "        return ans\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y=self.predict_proba(X)\n",
    "\n",
    "        # return the numpy array y which contains the predicted values\n",
    "        return y.argmax(axis=1)\n",
    "    \n",
    "    def cross_entropy_loss(self, A, y):\n",
    "        #check for problems\n",
    "\n",
    "        n = len(y)\n",
    "        logp = - np.log(A[np.arange(n), y.argmax(axis=1)]+1e-10)\n",
    "        loss = np.sum(logp+1e-10)/n\n",
    "        return loss\n",
    "    \n",
    "    def score(self, X , y_true):\n",
    "        y_pred=self.predict(X)\n",
    "        \n",
    "        return np.sum(y_true==y_pred)/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "pred\n",
      "Training Loss of epoch 0 is 2.2020917709978667\n",
      "Validation Loss of epoch 0 is 2.302570046279222\n",
      "Epoch 1\n",
      "pred\n",
      "Training Loss of epoch 1 is 2.2795696508674146\n",
      "Validation Loss of epoch 1 is 2.30258509122453\n",
      "Epoch 2\n",
      "pred\n",
      "Training Loss of epoch 2 is 2.3025850913605073\n",
      "Validation Loss of epoch 2 is 2.302585091223572\n",
      "Epoch 3\n",
      "pred\n",
      "Training Loss of epoch 3 is 2.3025850913597212\n",
      "Validation Loss of epoch 3 is 2.30258509122261\n",
      "Epoch 4\n",
      "pred\n",
      "Training Loss of epoch 4 is 2.302585091358933\n",
      "Validation Loss of epoch 4 is 2.302585091221643\n",
      "Epoch 5\n",
      "pred\n",
      "Training Loss of epoch 5 is 2.3025850913581394\n",
      "Validation Loss of epoch 5 is 2.302585091220671\n",
      "Epoch 6\n",
      "pred\n",
      "Training Loss of epoch 6 is 2.3025850913573365\n",
      "Validation Loss of epoch 6 is 2.302585091219694\n",
      "Epoch 7\n",
      "pred\n",
      "Training Loss of epoch 7 is 2.3025850913565273\n",
      "Validation Loss of epoch 7 is 2.302585091218712\n",
      "Epoch 8\n",
      "pred\n",
      "Training Loss of epoch 8 is 2.302585091355717\n",
      "Validation Loss of epoch 8 is 2.302585091217724\n",
      "Epoch 9\n",
      "pred\n",
      "Training Loss of epoch 9 is 2.3025850913548958\n",
      "Validation Loss of epoch 9 is 2.302585091216731\n",
      "Epoch 10\n",
      "pred\n",
      "Training Loss of epoch 10 is 2.302585091354074\n",
      "Validation Loss of epoch 10 is 2.3025850912157337\n",
      "Epoch 11\n",
      "pred\n",
      "Training Loss of epoch 11 is 2.3025850913532473\n",
      "Validation Loss of epoch 11 is 2.3025850912147297\n",
      "Epoch 12\n",
      "pred\n",
      "Training Loss of epoch 12 is 2.3025850913524235\n",
      "Validation Loss of epoch 12 is 2.30258509121372\n",
      "Epoch 13\n",
      "pred\n",
      "Training Loss of epoch 13 is 2.302585091351601\n",
      "Validation Loss of epoch 13 is 2.3025850912127037\n",
      "Epoch 14\n",
      "pred\n",
      "Training Loss of epoch 14 is 2.302585091350776\n",
      "Validation Loss of epoch 14 is 2.302585091211679\n",
      "Epoch 15\n",
      "pred\n",
      "Training Loss of epoch 15 is 2.3025850913499424\n",
      "Validation Loss of epoch 15 is 2.3025850912106463\n",
      "Epoch 16\n",
      "pred\n",
      "Training Loss of epoch 16 is 2.302585091349108\n",
      "Validation Loss of epoch 16 is 2.3025850912096075\n",
      "Epoch 17\n",
      "pred\n",
      "Training Loss of epoch 17 is 2.302585091348262\n",
      "Validation Loss of epoch 17 is 2.3025850912085626\n",
      "Epoch 18\n",
      "pred\n",
      "Training Loss of epoch 18 is 2.3025850913474137\n",
      "Validation Loss of epoch 18 is 2.3025850912075105\n",
      "Epoch 19\n",
      "pred\n",
      "Training Loss of epoch 19 is 2.302585091346578\n",
      "Validation Loss of epoch 19 is 2.302585091206449\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrs=[1]\n",
    "for i in lrs:\n",
    "    net=MyNeuralNetwork(5,[784,256,128,64,10],'tanh',i,'normal',64,20)\n",
    "    net.fit(train_images,train_labels,test_images,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fit(train_images,train_labels,test_images,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1135"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.score(test_images,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(256,128,64),random_state=1, max_iter=5, verbose=True).fit(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'softmax'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.out_activation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9786"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(test_images,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.0001,\n",
       " 'batch_size': 'auto',\n",
       " 'beta_1': 0.9,\n",
       " 'beta_2': 0.999,\n",
       " 'early_stopping': False,\n",
       " 'epsilon': 1e-08,\n",
       " 'hidden_layer_sizes': (256, 128, 64),\n",
       " 'learning_rate': 'constant',\n",
       " 'learning_rate_init': 0.001,\n",
       " 'max_fun': 15000,\n",
       " 'max_iter': 5,\n",
       " 'momentum': 0.9,\n",
       " 'n_iter_no_change': 10,\n",
       " 'nesterovs_momentum': True,\n",
       " 'power_t': 0.5,\n",
       " 'random_state': 1,\n",
       " 'shuffle': True,\n",
       " 'solver': 'adam',\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': True,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
